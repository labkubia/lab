{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, time\n",
    "\n",
    "\n",
    "class ShowProcess():\n",
    "    \"\"\"\n",
    "    显示处理进度的类\n",
    "    调用该类相关函数即可实现处理进度的显示\n",
    "    \"\"\"\n",
    "    i = 0 # 当前的处理进度\n",
    "    max_steps = 0 # 总共需要处理的次数\n",
    "    max_arrow = 50 #进度条的长度\n",
    "\n",
    "    # 初始化函数，需要知道总共的处理次数\n",
    "    def __init__(self, max_steps):\n",
    "        self.max_steps = max_steps\n",
    "        self.i = 0\n",
    "\n",
    "    # 显示函数，根据当前的处理进度i显示进度\n",
    "    # 效果为[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>]100.00%\n",
    "    def show_process(self, i=None):\n",
    "        if i is not None:\n",
    "            self.i = i\n",
    "        else:\n",
    "            self.i += 1\n",
    "        num_arrow = int(self.i * self.max_arrow / self.max_steps) #计算显示多少个'>'\n",
    "        num_line = self.max_arrow - num_arrow #计算显示多少个'-'\n",
    "        percent = self.i * 100.0 / self.max_steps #计算完成进度，格式为xx.xx%\n",
    "        process_bar = '[' + '>' * num_arrow + '-' * num_line + ']'\\\n",
    "                      + '%.2f' % percent + '%' + '\\r' #带输出的字符串，'\\r'表示不换行回到最左边\n",
    "        sys.stdout.write(process_bar) #这两句打印字符到终端\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    def close(self, words='done'):\n",
    "        print('done')\n",
    "        self.i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'density_func'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-f66f8fe52a9c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mSpeaker_identification\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSpeaker_identification\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdensity_func\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdensity_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnaive_G_U\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmixture\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGaussianMixture\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\LAB\\lab\\task_2_version_4\\Speaker_identification.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mdensity_func\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdensity_func\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mSpeaker_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnaive_G_U\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#math\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'density_func'"
     ]
    }
   ],
   "source": [
    "from Speaker_identification import Speaker_identification\n",
    "from density_func import density_func, naive_G_U\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#default\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "#loadmat\n",
    "from scipy.io import loadmat,savemat\n",
    "\n",
    "'''\n",
    "Import features\n",
    "'''\n",
    "\n",
    "save_path=\"D:\\\\LAB\\\\lab\\\\task_2_version_4\\\\features.txt\"\n",
    "#save_path=\"/Users/Mata/Documents/lab/task_2_version_3/features.txt\"\n",
    "f = open(save_path,'rb')\n",
    "features=pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "save_path=\"D:\\\\LAB\\\\lab\\\\task_2_version_4\\\\unknown_features.txt\"\n",
    "#save_path=\"/Users/Mata/Documents/lab/task_2_version_3/features.txt\"\n",
    "f = open(save_path,'rb')\n",
    "b_new_test=pickle.load(f)\n",
    "print(b_new_test.shape)\n",
    "f.close()\n",
    "\n",
    "'''\n",
    "Import UBM model\n",
    "'''\n",
    "\n",
    "#ubm_dataset=loadmat(\"/Users/Mata/Documents/2017/学习/ws2017:18/PUL/forStudents/ubm/UBM_GMMNaive_MFCC_Spectrum0to8000Hz.mat\",mat_dtype=True)\n",
    "ubm_dataset=loadmat(\"C:\\\\Users\\\\hasee\\\\workspace\\\\workspace\\\\lab\\\\patRecDat\\\\forStudents\\\\ubm\\\\UBM_GMMNaive_MFCC_Spectrum0to8000Hz.mat\",mat_dtype=True)\n",
    "ubm_means=ubm_dataset['means']\n",
    "ubm_var = ubm_dataset['var']\n",
    "ubm_weights = ubm_dataset['weights'].ravel()\n",
    "ubm_var_set=[]\n",
    "K_value=49\n",
    "gamma_UBM=1\n",
    "\n",
    "#transfer variance of UBM to cov\n",
    "for k in range(K_value):\n",
    "    ubm_var_set.append(np.diag(ubm_var[k]))\n",
    "ubm_var_set=np.array(ubm_var_set)\n",
    "ubm_var=ubm_var_set\n",
    "\n",
    "num_people=len(features.keys())\n",
    "\n",
    "'''\n",
    "Start Crossvalidation\n",
    "'''\n",
    "#process_bar=ShowProcess(10)\n",
    "detection_rate_set=[]\n",
    "start=time.time()\n",
    "error_set={}  \n",
    "num_samples=len(features.keys())\n",
    "name_set=list(features.keys())\n",
    "name_set.append(\"unknown\")\n",
    "confusion_matrix=np.zeros((num_samples,num_samples))\n",
    "correct_sum=0\n",
    "false_sum=0\n",
    "for cross_num in range(10):\n",
    "    #process_bar.show_process()\n",
    "    train_file_set=[]\n",
    "    test_file_set=[]\n",
    "    correct_num=0\n",
    "    false_num=0\n",
    "    '''\n",
    "    Split the test and train set\n",
    "    '''\n",
    "    for name in features.keys():\n",
    "        whole_set=features.get(name,'no such file').copy()\n",
    "        test_file=whole_set[cross_num]\n",
    "        train_num=list(range(10))\n",
    "        train_num.remove(cross_num)\n",
    "        test_file_set.append(test_file)\n",
    "        #name_set.append(name)\n",
    "        train_set=[]\n",
    "        for num in train_num:\n",
    "            train_set.append(whole_set[num])\n",
    "        train_set=np.concatenate(train_set,axis=1)\n",
    "        train_file_set.append(train_set)\n",
    "        \n",
    "    test_file_set.append(b_new_test)\n",
    "    \n",
    "    '''\n",
    "    Start modeling and identification\n",
    "    '''\n",
    "    print(\"crossvalidation \"+str(cross_num+1)+\" start\")\n",
    "    process_bar_2=ShowProcess(num_samples)\n",
    "    scores_set=np.zeros((num_samples+1,num_samples+1))\n",
    "    for index_2 in range(num_samples):\n",
    "        process_bar_2.show_process()\n",
    "        b_train=train_file_set[index_2]\n",
    "        gmm=GaussianMixture(n_components=K_value,covariance_type='full',max_iter=1,weights_init=ubm_weights,\\\n",
    "                            means_init=ubm_means,precisions_init=np.linalg.inv(ubm_var))\n",
    "        gmm.fit(b_train.T)\n",
    "        for index_1 in range(num_samples+1):\n",
    "            #print(\"now test set \"+str(index_1)+\" is testing \"+str(index_2))\n",
    "            b_test=np.array(test_file_set[index_1])\n",
    "            scores_set[index_1,index_2]=gmm.score(b_test.T)\n",
    "    '''\n",
    "    Enhancement : added the unknown detection part\n",
    "    '''\n",
    "    for index in range(num_samples+1):\n",
    "        b_test=np.array(test_file_set[index_1])\n",
    "        T_value=b_test.shape[1]\n",
    "        unknown_score=Speaker_identification(b_test,ubm_means,ubm_var,ubm_weights,T_value)\n",
    "        scores_set[index,num_samples+1]=unknown_score\n",
    "        \n",
    "        \n",
    "    '''\n",
    "    Calculate the detection rate\n",
    "    '''\n",
    "    for index in range(num_samples+1):\n",
    "        test_index=np.argwhere(scores_set[index,:]==max(scores_set[index,:]))\n",
    "        if index == test_index:\n",
    "            correct_num +=1\n",
    "            correct_sum +=1\n",
    "            confusion_matrix[index,index] +=1\n",
    "        else:\n",
    "            false_num +=1\n",
    "            false_sum +=1\n",
    "            confusion_matrix[index,test_index] +=1\n",
    "            print(\"error ! True: \"+name_set[index]+\" False: \"+name_set[test_index])\n",
    "        #print(\"time cost %5.1f second\"%((time.time()-start)/60))\n",
    "\n",
    "    process_bar_2.close()\n",
    "\n",
    "    detection_rate=correct_num/(correct_num+false_num)\n",
    "    print(\"crossvalidation \"+str(cross_num+1)+\" compeleted\")\n",
    "    print(\"cost time %5.1f minute\"%((time.time()-start)/60))\n",
    "    detection_rate=correct_num/(false_num+correct_num)\n",
    "    detection_rate_set.append(detection_rate)\n",
    "    print(\"the crossval \"+str(cross_num)+\" detection_rate is \"+str(detection_rate))\n",
    "\n",
    "print(\"the total detection rate is \",correct_sum/(correct_sum+false_sum))\n",
    "\n",
    "save_path=\"D:\\\\LAB\\\\lab\\\\task_2_version_4\\\\confusion_matrix.txt\"\n",
    "#save_path=\"/Users/Mata/Documents/lab/task_2_version_3/features.txt\"\n",
    "f = open(save_path,'wb')\n",
    "features=pickle.dump(confusion_matrix,f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "#loadmat\n",
    "from scipy.io import loadmat,savemat\n",
    "\n",
    "'''\n",
    "Import features\n",
    "'''\n",
    "\n",
    "save_path=\"D:\\\\LAB\\\\lab\\\\task_2_version_3\\\\features.txt\"\n",
    "#save_path=\"/Users/Mata/Documents/lab/task_2_version_3/features.txt\"\n",
    "f = open(save_path,'rb')\n",
    "features=pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
